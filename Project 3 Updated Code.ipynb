{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04fd1379-7f78-488e-b2fd-2acde3ffdbc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout, LSTM, Bidirectional\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.initializers import Constant\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import hamming_loss, f1_score, precision_score, recall_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d18850-3e50-4e28-850c-1b8f1692a3ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:86: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m778/778\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1028s\u001b[0m 1s/step - accuracy: 0.4237 - loss: 1.8050 - val_accuracy: 0.5774 - val_loss: 1.1030\n",
      "Epoch 2/10\n",
      "\u001b[1m778/778\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m803s\u001b[0m 1s/step - accuracy: 0.6453 - loss: 1.1030 - val_accuracy: 0.6848 - val_loss: 0.8816\n",
      "Epoch 3/10\n",
      "\u001b[1m778/778\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m743s\u001b[0m 955ms/step - accuracy: 0.6623 - loss: 1.0402 - val_accuracy: 0.6191 - val_loss: 0.9974\n",
      "Epoch 4/10\n",
      "\u001b[1m778/778\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m762s\u001b[0m 979ms/step - accuracy: 0.6749 - loss: 0.9935 - val_accuracy: 0.5681 - val_loss: 1.0992\n",
      "Epoch 5/10\n",
      "\u001b[1m778/778\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m761s\u001b[0m 978ms/step - accuracy: 0.6798 - loss: 0.9600 - val_accuracy: 0.6278 - val_loss: 0.9516\n",
      "\u001b[1m5017/5330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m9s\u001b[0m 29ms/step"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the dataset\n",
    "df_raw = pd.read_csv('Reviews.csv')\n",
    "df = df_raw[['Score', 'Text']].copy()\n",
    "\n",
    "# Preprocess text\n",
    "def clean_text(text):\n",
    "    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text, re.I | re.A)\n",
    "    text = text.lower().strip()\n",
    "    return text\n",
    "\n",
    "df.loc[:, 'Clean_Text'] = df['Text'].apply(clean_text)  # Avoid SettingWithCopyWarning\n",
    "\n",
    "# Tokenization and padding\n",
    "max_words = 20000\n",
    "max_len = 200\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(df['Clean_Text'])\n",
    "sequences = tokenizer.texts_to_sequences(df['Clean_Text'])\n",
    "x = pad_sequences(sequences, maxlen=max_len)\n",
    "\n",
    "# One-hot encode labels\n",
    "y = df[['Score']].values\n",
    "encoder = OneHotEncoder(sparse_output=False)  # Updated parameter\n",
    "y_one_hot_encode = encoder.fit_transform(y)\n",
    "\n",
    "# Train-test split\n",
    "x_train, x_val, y_train, y_val = train_test_split(x, y_one_hot_encode, test_size=0.3, random_state=42)\n",
    "\n",
    "# Handle class imbalance\n",
    "class_weights = class_weight.compute_class_weight(\n",
    "    'balanced', classes=np.unique(y_train.argmax(axis=1)), y=y_train.argmax(axis=1)\n",
    ")\n",
    "class_weights_dict = {i: weight for i, weight in enumerate(class_weights)}\n",
    "\n",
    "# Load pretrained embeddings (GloVe example)\n",
    "embedding_dim = 100\n",
    "embedding_index = {}\n",
    "with open(\"glove.6B.100d.txt\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        word, *vector = line.split()\n",
    "        embedding_index[word] = np.array(vector, dtype=\"float32\")\n",
    "\n",
    "# Create embedding matrix\n",
    "word_index = tokenizer.word_index\n",
    "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    if i < max_words:\n",
    "        embedding_vector = embedding_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "\n",
    "# Build CNN + BiLSTM model\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=max_words,\n",
    "              output_dim=embedding_dim,\n",
    "              input_length=max_len,\n",
    "              trainable=True),  # Trainable embedding layer without pretrained weights\n",
    "    Conv1D(filters=128, kernel_size=5, activation='relu', kernel_regularizer='l2'),\n",
    "    Dropout(0.5),\n",
    "    Bidirectional(LSTM(64, return_sequences=False)),\n",
    "    Dense(128, activation='relu', kernel_regularizer='l2'),\n",
    "    Dropout(0.5),\n",
    "    Dense(5, activation='softmax')  # For multiclass classification\n",
    "])\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model with early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "history = model.fit(\n",
    "    x_train, y_train,\n",
    "    epochs=10,\n",
    "    batch_size=512,\n",
    "    validation_data=(x_val, y_val),\n",
    "    class_weight=class_weights_dict,\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred_prob = model.predict(x_val)\n",
    "y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "y_true = np.argmax(y_val, axis=1)\n",
    "\n",
    "# Calculate metrics\n",
    "hamming = hamming_loss(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred, average='macro')\n",
    "precision = precision_score(y_true, y_pred, average='macro')\n",
    "recall = recall_score(y_true, y_pred, average='macro')\n",
    "conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "print(f\"Hamming Loss: {hamming}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# Visualize misclassified samples\n",
    "misclassified_idx = np.where(y_true != y_pred)[0][:5]\n",
    "for idx in misclassified_idx:\n",
    "    raw_sentence = [i for i in tokenizer.sequences_to_texts([x_val[idx]])[0].split(' ') if i != 'UNK']\n",
    "    print(f\"Predicted: {y_pred[idx]}, Actual: {y_true[idx]}, Sentence: {raw_sentence}\")\n",
    "\n",
    "# Plot training vs validation metrics\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Accuracy Over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Loss Over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
